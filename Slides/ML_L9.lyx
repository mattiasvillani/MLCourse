#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble

% you can play with different themes and color themes to find your favorite combination.
\mode<presentation> {
  \usetheme{Luebeck}
  \usecolortheme{beaver}
  \beamertemplatenavigationsymbolsempty
  \setbeamertemplate{headline}{}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% include necessary packages here
\usepackage{graphicx} % for including images
\usepackage{pgf} % for logo
\usepackage{colortbl}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\date{} % Date, can be changed to a custom date

\titlegraphic{

\includegraphics[width=1.5cm]{Images/LogoBlueJustRing.jpg}\hspace*{2.5cm}~%
\includegraphics[width=2cm]{Images/liulogo.png} \linebreak
\hrulefill \break
\tiny
\includegraphics[width=0.33cm]{Images/web.png} \href{https://mattiasvillani.com}{mattiasvillani.com}\hspace*{1cm}~
\includegraphics[width=0.3cm]{Images/twitter.jpg} \href{https://twitter.com/matvil}{@matvil}\hspace*{1cm}~
\includegraphics[width=0.3cm]{Images/github.png} \href{https://github.com/mattiasvillani}{mattiasvillani}~
}


\definecolor{blue}{RGB}{38, 122, 181}
\definecolor{orange}{RGB}{255, 128, 0}
\definecolor{lorange}{RGB}{255, 178, 102}
\definecolor{llorange}{RGB}{255, 229,204 }
\definecolor{red}{RGB}{255, 128, 0}
\definecolor{verylightgray}{RGB}{246, 246, 246}


\setbeamertemplate{itemize item}{\color{orange}$\blacksquare$}
\setbeamertemplate{itemize subitem}{\color{orange}$\blacktriangleright$}

\usepackage{tcolorbox}

\usepackage[ruled]{algorithm2e}
\usepackage{wasysym}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}
\end_preamble
\options xcolor=svgnames, handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "palatino" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title

\color orange
Machine Learning
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\color gray
Machine Learning
\end_layout

\end_inset


\end_layout

\begin_layout Subtitle

\color orange
Lecture 9 - Unsupervised learning, mixture models and clustering
\end_layout

\begin_layout Author

\series bold
Mattias Villani
\series default
 
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout

\series bold
\color gray
Mattias Villani
\end_layout

\end_inset


\end_layout

\begin_layout Institute
Department of Statistics
\begin_inset Newline newline
\end_inset

Stockholm University 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Department of Computer and Information Science
\begin_inset Newline newline
\end_inset

Linköping University 
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Linköping and Stockholm University
\end_layout

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Lecture overview
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
Supervised Mixture-of-Normals
\series default
\color inherit

\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Unsupervised Mixture-of-Normals
\series default
\color inherit

\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
k-means clustering
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Multivariate normal distribution
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\mathcal{\mathbf{x}\in\mathbb{R}}^{p}$
\end_inset

 is a 
\series bold
\color blue
multivariate normal
\series default
\color inherit
, 
\begin_inset Formula $\mathbf{x}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$
\end_inset

, with 
\series bold
\color blue
density
\series default
\color inherit

\begin_inset Formula 
\[
p(\mathbf{x})=\left|2\pi\boldsymbol{\Sigma}\right|^{-1/2}\exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Mean
\series default
\color inherit
 and 
\series bold
\color blue
covariance
\series default
\color inherit
 matrix
\begin_inset Formula 
\[
\mathbb{E}(\boldsymbol{x})=\boldsymbol{\mu}\quad\text{ and }\quad\mathrm{Cov}(\boldsymbol{x})=\boldsymbol{\Sigma}
\]

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename /home/mv/Dropbox/BayesBook/Figs/MultiNormal1.pdf
	scale 18

\end_inset


\begin_inset Graphics
	filename /home/mv/Dropbox/BayesBook/Figs/MultiNormal2.pdf
	scale 18

\end_inset


\begin_inset Graphics
	filename /home/mv/Dropbox/BayesBook/Figs/MultiNormal3.png
	scale 18

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Multivariate normal distribution - properties
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\mathcal{\mathbf{x}\in\mathbb{R}}^{p}$
\end_inset

 is 
\series bold
\color blue
multivariate normal
\series default
\color inherit
, 
\begin_inset Formula $\mathbf{x}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$
\end_inset


\series bold
\color blue
 
\series default
\color inherit
with
\begin_inset Formula 
\[
\mathbb{E}(\boldsymbol{x})=\boldsymbol{\mu}\text{ and }\mathrm{Cov}(\boldsymbol{x})=\boldsymbol{\Sigma}
\]

\end_inset


\end_layout

\begin_layout Itemize
Decompose 
\begin_inset Formula 
\[
\mathbf{x}=\left(\begin{array}{c}
\boldsymbol{x}_{1}\\
\boldsymbol{x}_{2}
\end{array}\right),\boldsymbol{\mu}=\left(\begin{array}{c}
\boldsymbol{\mu}_{1}\\
\boldsymbol{\mu}_{2}
\end{array}\right)\text{ and }\boldsymbol{\Sigma}=\left(\begin{array}{cc}
\boldsymbol{\Sigma}_{11} & \boldsymbol{\Sigma}_{12}\\
\boldsymbol{\Sigma}_{21} & \boldsymbol{\Sigma}_{22}
\end{array}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Marginal distributions
\series default
\color inherit
 are normal 
\begin_inset Formula 
\[
\mathbf{x}_{1}\sim\mathcal{N}(\boldsymbol{\mu}_{1},\boldsymbol{\Sigma}_{11})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Conditional distributions
\series default
\color inherit
 are normal 
\begin_inset Formula 
\[
\mathbf{x}_{1}\vert\boldsymbol{x}_{2}\sim\mathcal{N}(\tilde{\boldsymbol{\mu}}_{1},\tilde{\boldsymbol{\Sigma}}_{1})
\]

\end_inset

with
\begin_inset Formula 
\begin{align*}
\tilde{\boldsymbol{\mu}}_{1} & =\boldsymbol{\mu}_{1}+\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}(\boldsymbol{x}_{2}-\boldsymbol{\mu}_{2})\\
\tilde{\boldsymbol{\Sigma}}_{1} & =\boldsymbol{\Sigma}_{11}-\boldsymbol{\Sigma}_{12}\boldsymbol{\Sigma}_{22}^{-1}\boldsymbol{\Sigma}_{21}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Discriminative vs Generative models
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
Discriminative models
\series default
\color inherit
: modeling of 
\begin_inset Formula $p(y\vert\mathbf{x})$
\end_inset

.
 No model for 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Itemize
Logistic regression
\end_layout

\begin_layout Itemize
Regression trees and ensembles
\end_layout

\begin_layout Itemize
Deep neural networks
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\color blue
Generative models
\series default
\color inherit
: 
\emph on
joint
\emph default
 distribution of labels and features
\begin_inset Formula 
\[
p(y,\mathbf{x})=p(\mathbf{x}\vert y)p(y).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Discriminant analysis
\end_layout

\begin_layout Itemize
Mixture models
\end_layout

\end_deeper
\begin_layout Itemize
Generative models require more effort: 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
need also a model for 
\begin_inset Formula $\boldsymbol{x}$
\end_inset


\end_layout

\begin_layout Itemize
features may be high-dimensional.
 Hard!
\end_layout

\end_deeper
\begin_layout Itemize
Generative models
\series bold
\color blue
:
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
better understanding
\series default
\color inherit
 of the mechanisms.
 
\end_layout

\begin_layout Itemize
can be extended to 
\series bold
\color blue
unsupervised
\series default
\color inherit
 (all labels missing) and 
\series bold
\color blue
semi-supervised
\series default
\color inherit
 (some labels missing).
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Gaussian mixture model - supervised case
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\mathcal{N}(\text{x}|\boldsymbol{\mu},\boldsymbol{\Sigma})$
\end_inset

 denote the density function of 
\begin_inset Formula $\mathbf{x}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$
\end_inset


\end_layout

\begin_layout Itemize
Joint model for label 
\begin_inset Formula $y\in\{1,\ldots,M\}$
\end_inset

 and features 
\begin_inset Formula $\mathbf{x}\in\mathbb{R}^{p}$
\end_inset

:
\begin_inset Formula 
\[
p(y,\mathbf{x})=p(\mathbf{x}\vert y)p(y)
\]

\end_inset

where the 
\series bold
\color blue
class-conditional densities
\series default
\color inherit
 are normal:
\begin_inset Formula 
\[
p(\mathbf{x}\vert y)=\mathcal{N}(\text{x}|\boldsymbol{\mu}_{y},\boldsymbol{\Sigma}_{y})
\]

\end_inset


\end_layout

\begin_layout Itemize
Known labels: 
\series bold
\color blue
MLE for each class separately
\series default
\color inherit

\begin_inset Formula 
\begin{align*}
\hat{\boldsymbol{\mu}}_{m} & =\frac{1}{n_{m}}\sum_{i:y_{i}=m}\boldsymbol{x}_{i}\\
\hat{\boldsymbol{\Sigma}}_{m} & =\frac{1}{n_{m}}\sum_{i:y_{i}=m}(\boldsymbol{x}_{i}-\hat{\boldsymbol{\mu}}_{m})(\boldsymbol{x}_{i}-\hat{\boldsymbol{\mu}}_{m})^{\top}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
MLE for discrete labels
\series default
\color inherit
 
\begin_inset Formula $p(y=m)=p_{m}$
\end_inset

 
\begin_inset Formula 
\[
\hat{p}_{m}=\frac{n_{m}}{n}.
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Discriminant analysis
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
Classification probability
\series default
\color inherit
 by Bayes' theorem
\size footnotesize

\begin_inset Formula 
\[
p(y\vert\mathbf{x})=\frac{p(y,\mathbf{x})}{p(\mathbf{x})}=\frac{p(\mathbf{x}\vert y)p(y)}{p(\mathbf{x})}\propto p(\mathbf{x}\vert y)p(y)
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Quadratic discriminant analysis 
\series default
\color inherit
(
\series bold
\color blue
QDA
\series default
\color inherit
)
\size footnotesize

\begin_inset Formula 
\begin{align*}
\arg\underset{m}{\max} & \left(\log\hat{p}(\mathbf{x}_{\star}\vert y=m)+\log\hat{p}(y=m)\right)\\
= & \arg\underset{m}{\max}\left(\log\mathcal{N}(\mathbf{x}_{\star}|\hat{\boldsymbol{\mu}}_{m},\hat{\boldsymbol{\Sigma}}_{m})+\log\hat{p}_{m}\right)\\
= & \arg\underset{m}{\max}\left(-\frac{1}{2}\log\left|\hat{\boldsymbol{\Sigma}}_{m}\right|-\frac{1}{2}(\mathbf{x}_{\star}-\hat{\boldsymbol{\mu}}_{m})^{\top}\hat{\boldsymbol{\Sigma}}_{m}^{-1}(\mathbf{x}_{\star}-\hat{\boldsymbol{\mu}}_{m})+\log\hat{p}_{m}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
So QDA has a quadratic decision boundary.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Linear discriminant analysis
\series default
\color inherit
 (
\series bold
\color blue
LDA
\series default
\color inherit
) if we assume 
\size footnotesize

\begin_inset Formula 
\[
\boldsymbol{\Sigma}_{1}=\boldsymbol{\Sigma}_{2}=\ldots=\boldsymbol{\Sigma}_{M}=\boldsymbol{\Sigma}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Discriminant analysis
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/SongClassficationDiscrAnalysis.png
	scale 23

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
blfootnote{
\backslash
tiny Figure from Lindholm et al (2021).}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Gaussian mixture model - unsupervised case
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
Unsupervised
\series default
\color inherit
: no labels.
 
\end_layout

\begin_layout Itemize
Don't know from which distribution 
\begin_inset Formula $\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m})$
\end_inset

 each observation 
\begin_inset Formula $\boldsymbol{x}_{i}$
\end_inset

 comes from.
\end_layout

\begin_layout Itemize

\series bold
\color blue
Two-component univariate mixture of normals
\series default
\color inherit

\begin_inset Formula 
\[
p(x)=\pi\cdot\mathcal{N}(x|\mu_{1},\sigma_{1}^{2})+(1-\pi)\cdot\mathcal{N}(x|\mu_{2},\sigma_{2}^{2})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Simulate
\series default
\color inherit
 from a two-component mixture of normals:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Simulate 
\begin_inset Formula $y_{i}\in\{1,2\}$
\end_inset

, with 
\begin_inset Formula $\mathrm{Pr}(y_{i}=1)=\pi$
\end_inset

.
\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y_{i}=1$
\end_inset

, simulate 
\begin_inset Formula $x$
\end_inset

 from 
\begin_inset Formula $N(\mu_{1},\sigma_{1}^{2})$
\end_inset


\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y_{i}=2$
\end_inset

, simulate 
\begin_inset Formula $x$
\end_inset

 from 
\begin_inset Formula $N(\mu_{2},\sigma_{2}^{2}).$
\end_inset


\begin_inset VSpace smallskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\color blue
\begin_inset Formula $M$
\end_inset

-component mixture of multivariate normals
\series default
\color inherit

\begin_inset Formula 
\[
p(\boldsymbol{x})=\sum_{m=1}^{M}\pi_{m}\mathcal{N}(\boldsymbol{x}\vert\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m}),\quad\sum\nolimits _{m=1}^{M}\pi_{m}=1
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Illustration of mixture distributions
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/MixtureOfNormals.eps
	scale 32

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Fish length - Histogram density estimates
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/FishHistDensEst.png
	lyxscale 30
	scale 25

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Fish length - Kernel density estimates
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/FishKernelDensEst.png
	lyxscale 30
	scale 25

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Fish length - Mixture of normals
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/FishMoN.png
	lyxscale 30
	scale 23

\end_inset


\end_layout

\begin_layout Itemize
See code 
\family typewriter
GMM_EM.R
\family default
 on web page.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
EM algorithms for unsupervised Gaussian mixtures
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
GMM - the likelihood is a messy product of sums
\size footnotesize

\begin_inset Formula 
\[
p(\boldsymbol{x}_{1},...,\boldsymbol{x}_{n}\vert\boldsymbol{\mu}_{1:M},\boldsymbol{\Sigma}_{1:M},\pi_{1:M})=\prod_{i=1}^{n}\sum_{m=1}^{M}\pi_{m}\mathcal{N}(\boldsymbol{x}_{i}\vert\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m}).
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Complete data likelihood
\series default
\color inherit
:
\size footnotesize

\begin_inset Formula 
\[
p(\boldsymbol{x}_{1},...,\boldsymbol{x}_{n},y_{1},\ldots,y_{n}\vert\boldsymbol{\mu}_{1:M},\boldsymbol{\Sigma}_{1:M},\pi_{1:M})=\prod_{i=1}^{n}\pi_{y_{i}}\mathcal{N}(\boldsymbol{x}_{i}\vert\boldsymbol{\mu}_{y_{i}},\boldsymbol{\Sigma}_{y_{i}}).
\]

\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\boldsymbol{\theta}=(\boldsymbol{\mu}_{1:M},\boldsymbol{\Sigma}_{1:M},\pi_{1:M})$
\end_inset

 be all model parameters.
\end_layout

\begin_layout Itemize
Iterative
\series bold
\color blue
 EM-algorithm
\series default
\color inherit
 for MLE.
 Given previous estimate 
\begin_inset Formula $\hat{\boldsymbol{\theta}}$
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\color blue
E-step
\series default
\color inherit
: 
\begin_inset Formula $\text{Compute }Q(\boldsymbol{\theta})\equiv\mathbb{E}_{\boldsymbol{y}}\left(\log p(\boldsymbol{X},\boldsymbol{y}\vert\boldsymbol{\theta})\vert\boldsymbol{X},\hat{\boldsymbol{\theta}}\right)$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
M-step
\series default
\color inherit
: 
\begin_inset Formula $\text{Update }\hat{\boldsymbol{\theta}}\leftarrow\arg\underset{\boldsymbol{\theta}}{\max}\;Q(\boldsymbol{\theta})$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
The 
\series bold
\color blue
expected log-likelihood
\series default
\color inherit
 is
\size footnotesize

\begin_inset Formula 
\[
Q(\boldsymbol{\theta})=\sum_{i=1}^{n}\sum_{m=1}^{M}w_{i}(m)\left[\log\mathcal{N}(\boldsymbol{x}_{i}\vert\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m})+\log\pi_{m}\right]
\]

\end_inset


\size default
where 
\begin_inset Formula $w_{i}(m)\equiv p\left(y_{i}=m\vert\hat{\boldsymbol{\theta}},\boldsymbol{x}_{i}\right).$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
EM-algorithm for a Gaussian mixture model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\size footnotesize
(
\series bold
\color blue
E-step
\series default
\color inherit
) Compute probabilities for the latent 
\begin_inset Formula $y_{i}$
\end_inset


\begin_inset Formula 
\[
w_{i}(m)=\frac{\pi_{m}\mathcal{N}(\boldsymbol{x}_{i}\vert\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m})}{\sum_{j=1}^{M}\pi_{j}\mathcal{N}(\boldsymbol{x}_{i}\vert\boldsymbol{\mu}_{j},\boldsymbol{\Sigma}_{j})}\quad\text{ and }\quad\hat{n}_{m}=\sum_{i=1}^{n}w_{i}(m)
\]

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
(
\series bold
\color blue
M-step
\series default
\color inherit
) Given 
\begin_inset Formula $w_{i}(m)$
\end_inset

, compute ML estimates by maximizing 
\begin_inset Formula $Q(\boldsymbol{\theta})$
\end_inset

:
\begin_inset Newline newline
\end_inset


\begin_inset Formula 
\begin{align*}
\hat{\pi}_{m} & =\frac{\hat{n}_{m}}{n}\\
\hat{\boldsymbol{\mu}}_{m} & =\frac{1}{\hat{n}_{m}}\sum_{i=1}^{n}w_{i}(m)\boldsymbol{x}_{i}\\
\hat{\boldsymbol{\Sigma}}_{m} & =\frac{1}{\hat{n}_{m}}\sum_{i=1}^{n}w_{i}(m)(\boldsymbol{x}_{i}-\hat{\boldsymbol{\mu}}_{m})(\boldsymbol{x}_{i}-\hat{\boldsymbol{\mu}}_{m})^{\top}
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize

\size footnotesize
Iterate until log-likelihood 
\begin_inset Formula 
\[
\sum_{i=1}^{N}\log\left\{ \sum_{m=1}^{M}\pi_{m}\mathcal{N}(\boldsymbol{x}_{i}\vert\boldsymbol{\mu}_{m},\boldsymbol{\Sigma}_{m})\right\} 
\]

\end_inset

satisfies some stopping rule.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Gaussian mixture model for old Faithful data
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/EMMoNOldFaithful.jpeg
	lyxscale 10
	scale 8

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
blfootnote{
\backslash
tiny Figure from Bishop (2011).}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
EM-algorithm for a Gaussian mixture model
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Log-likelihood is 
\series bold
\color blue
guaranteed to not decrease
\series default
\color inherit
 at any iteration
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
EM is typically 
\series bold
\color blue
slow
\series default
\color inherit
.
 
\begin_inset Newline newline
\end_inset

Switch to Newton-Raphson when closer to the maximum.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
EM solution often depends on the 
\series bold
\color blue
initial values
\series default
\color inherit
.
 
\begin_inset Newline newline
\end_inset

Restart with different initial values and pick the best solution.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
\color blue
Label-switching
\series default
\color inherit
 for mixtures.
 There are actually 
\begin_inset Formula $M!$
\end_inset

 maxima.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
EM tends to find a finite maximum even though the 
\series bold
\color blue
likelihood is unbounded
\series default
\color inherit
.
 Singularities.
 Likelihood becomes arbitrarily large if mth component is 
\begin_inset Formula $\mathcal{N}(\boldsymbol{x}_{j},\boldsymbol{\Sigma}_{m})$
\end_inset

 with 
\begin_inset Formula $\boldsymbol{\Sigma}_{m}\rightarrow\boldsymbol{0}$
\end_inset

.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
(pssst: use a Bayesian prior!)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
k-Means clustering
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Data in 
\begin_inset Formula $\mathbb{R}^{p}$
\end_inset

: 
\begin_inset Formula $\mathbf{x}_{1},...,\mathbf{x}_{n}$
\end_inset

.
\end_layout

\begin_layout Itemize
Aim: 
\series bold
\color blue
partition
\color inherit
 
\series default
the data into
\series bold
 
\begin_inset Formula $M$
\end_inset

 
\color blue
clusters
\series default
\color inherit
.
\end_layout

\begin_layout Itemize
Each observation 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is represented by a 
\series bold
\color blue
centroid
\series default
\color inherit
 
\begin_inset Formula $\boldsymbol{\mu}_{m}\in\mathbb{R}^{p}$
\end_inset

.
\end_layout

\begin_layout Itemize
Let responsibility 
\begin_inset Formula $r_{im}=1$
\end_inset

 if 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 belongs to 
\begin_inset Formula $\boldsymbol{\mu}_{m}$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
\color blue
k-means clustering
\series default
\color inherit
 minimizes
\begin_inset Formula 
\[
\sum_{i=1}^{n}\sum_{m=1}^{M}r_{im}\left\Vert \mathbf{x}_{i}-\boldsymbol{\mu}_{m}\right\Vert ^{2}
\]

\end_inset

with respect to the 
\begin_inset Formula $r_{im}$
\end_inset

 and the 
\begin_inset Formula $\boldsymbol{\mu}_{1},...,\boldsymbol{\mu}_{M}$
\end_inset

.
\end_layout

\begin_layout Itemize
Iterative algorithm: initialize 
\begin_inset Formula $\boldsymbol{\mu}_{1},...,\boldsymbol{\mu}_{M}$
\end_inset

, then:
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Allocate each observations to nearest centroid (
\begin_inset Formula $r_{im}=1$
\end_inset

) 
\end_layout

\begin_layout Itemize
Recompute each centroid as mean of its allocated observations.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
k-Means clustering of old faithful data
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/kMeansOldFaithful.jpeg
	lyxscale 10
	scale 6

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
blfootnote{
\backslash
tiny Figure from Bishop (2011).}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Clusterings songs by k-means
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/SongClassifyKMeans.png
	lyxscale 30
	scale 25

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
blfootnote{
\backslash
tiny Figure from Lindholm et al.
 (2021).}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Selecting the number of clusters - elbow
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/kmeansElbow.png
	lyxscale 25
	scale 19

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
blfootnote{
\backslash
tiny Figure from Lindholm et al.
 (2021).}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout

\series bold
\color orange
Image compression by k-means
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Images/kMeansImageCompress.jpeg
	lyxscale 10
	scale 8

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
blfootnote{
\backslash
tiny Figure from Bishop (2011).}
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document
